{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value must have type '<class 'int'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5f019e2ccead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_columns'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.expand_frame_repr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max_colwidth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\tallamrahul\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_config\\config.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tallamrahul\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_config\\config.py\u001b[0m in \u001b[0;36m_set_option\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_registered_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# walk the nested dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tallamrahul\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_config\\config.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Value must have type '{typ!s}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Value must have type '<class 'int'>'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It requires python 3.8 or higher\n",
    "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It spent 3 hours for scraping more than 180,000 tweets\n",
    "text_query = \"$NFLX\"\n",
    "since_date = \"2018-01-01\"\n",
    "until_date = \"2022-07-11\"\n",
    "os.system('snscrape --jsonl --since {} twitter-search \"{} until:{}\"> text-query-tweets.json'.format(since_date, text_query, until_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading jason files as dataframes\n",
    "tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "# tweets_df2 = pd.read_json('text-query-tweets2.json', lines=True)\n",
    "# Merging both dataframes as a single dataframe\n",
    "# tweets_df = pd.concat([tweets_df1,tweets_df2],ignore_index=True)\n",
    "# Selecting the important columns only wich are Data,renderContent and Lang\n",
    "tweets_content = tweets_df.loc[:,['date','renderedContent','lang']]\n",
    "# Choosing the tweets in english language only\n",
    "tweets_content = tweets_content[tweets_content['lang']=='en']\n",
    "# Dropping the lang column\n",
    "tweets_content.drop(\"lang\",axis=1,inplace=True)\n",
    "# Download the CSV file result on the current folder.\n",
    "tweets_content.to_csv('Ntweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install demoji\n",
    "import demoji\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag\n",
    "import attr\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to pass the POS tage for each word passed through clean_text function\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning tweets\n",
    "def clean_text(text):\n",
    "    # Initialization the twitter tokenizer\n",
    "    tk = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True) \n",
    "    # Initialization the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    # Trying to avoid deleting the negative verbs as it affects the meaning of the tweets.\n",
    "    stop_words = stopwords.words('english') + [\"i'll\",\"i'm\", \"should\", \"could\"]\n",
    "    negative_verbs = [ \"shan't\",'shouldn',\"shouldn't\",'wasn','weren','won','wouldn','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn',\"mustn't\",'needn',\"needn't\",\"wouldn't\",\"won't\",\"weren't\",\"wasn't\",\"couldn\",\"not\",\"nor\",\"no\",\"mightn't\",\"isn't\",\"haven't\",\"hadn't\",\"hasn't\",\"didn't\",\"doesn't\",\"aren't\",\"don't\",\"couldn't\",\"never\"]\n",
    "    stop_words =[word for word in stop_words if word not in negative_verbs ] \n",
    "    \n",
    "    # Lowering tweets\n",
    "    lower_tweet = text.lower() \n",
    "    # Removing hashtag and cashtag symbols\n",
    "    tweet = re.sub(r\"[#$]\",\" \",lower_tweet)\n",
    "    # Removing links from tweets\n",
    "    tweet = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\",\" \", tweet)\n",
    "    # Translating emojies into thier descriptions\n",
    "    tweet = demoji.replace_with_desc(tweet)\n",
    "    # removing numerical values\n",
    "    tweet = re.sub(r\"[0-9]|-->\",\"\",tweet)\n",
    "    # Tokenize the tweets by twitter tokenzier.\n",
    "    tweet = tk.tokenize(tweet)\n",
    "    # Choosing the words that don't exist in stopwords, thier lengths are more than 2 letters and then lemmatize them.\n",
    "    tweet = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tweet if word not in stop_words and word not in string.punctuation and len(word)>2 and \".\" not in word]\n",
    "    # return the tokens in one sentence \n",
    "    tweet = \" \".join(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scrapped file\n",
    "tweets = pd.read_csv(\"Ntweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text cleaning and then downloading it on the current folder\n",
    "tweets['cleaned'] = tweets[\"renderedContent\"].apply(lambda row:clean_text(row))\n",
    "tweets.to_csv(\"CleanedNTweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"CleanedNTweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "# TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    # Print labels and scores\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    l = config.id2label[ranking[0]]\n",
    "    plrty = -1 if l == \"Negative\" else 1 if l == \"Positive\" else 0 \n",
    "    s = np.round(float(scores[ranking[0]]), 4)\n",
    "    return (l,plrty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the file after applying sentiment analysis on the current folder\n",
    "tweets['label'],tweets['Polarity'] = zip(*tweets['cleaned'].apply(lambda txt:polarity(txt)))\n",
    "tweets.to_csv(\"polarizedTweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptweets = pd.read_csv(\"polarizedTweets.csv\")\n",
    "ptweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the date and polarized values from the previous dataframe\n",
    "ptweets_df = ptweets.loc[:,[\"date\",\"Polarity\"]]\n",
    "ptweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the date format to match with the next csv file date format\n",
    "ptweets_df['date'] =pd.to_datetime(ptweets_df['date'],infer_datetime_format=True)\n",
    "ptweets_df['date'] =pd.to_datetime(ptweets_df['date'].dt.strftime(\"%m/%d/%y\"))\n",
    "\n",
    "# Aggregate the tweets polarization by avergae, sum and counts \n",
    "Pol_df = pd.DataFrame(ptweets_df.groupby('date')['Polarity'].mean())\n",
    "Pol_df.rename(columns={\"Polarity\":\"P_mean\"},inplace=True)\n",
    "Pol_df['P_sum'] = ptweets_df.groupby('date')['Polarity'].sum()\n",
    "Pol_df['twt_count'] = ptweets_df.groupby('date')['Polarity'].count()\n",
    "Pol_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the netflix finance data and preparing it to fit with the polarized values\n",
    "nflx_df = pd.read_csv(\"NFLX.csv\")\n",
    "nflx_df.rename(columns={\"Date\":\"date\"},inplace=True)\n",
    "nflx_df['date'] = pd.to_datetime(nflx_df['date'],infer_datetime_format=True)\n",
    "nflx_df.set_index(\"date\")\n",
    "# Adding the polarization column in the netflix dataframe.\n",
    "final_df = nflx_df.join(Pol_df,on='date',how=\"inner\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the final CSV file that has the finance data and tweets polarizations\n",
    "final_df.to_csv(\"FinalNflx.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018=pd.read_csv('nflx2018-2020.csv')\n",
    "df_2020=pd.read_csv('nflx2020-2022.csv')\n",
    "df_all=pd.concat([df_2018,df_2020])\n",
    "df_all=df_all.reset_index()\n",
    "df_all.drop('index',inplace=True,axis=1)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('Final_nflx_data_2018-2022',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sss=pd.read_csv('CleanedNTweets.csv')\n",
    "sss.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sss=pd.read_csv('NFLX.csv')\n",
    "sss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sss=pd.read_csv('Final_nflx_data_2018-2022')\n",
    "sss.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
